#  Projet DataFlow360 ‚Äì Collecte & Ingestion Multi-Source

##  Contexte du projet

**DataFlow360** est une plateforme de traitement et d‚Äôanalyse de donn√©es a√©riennes et m√©t√©orologiques.  
Cette premi√®re phase du projet met l‚Äôaccent sur la **collecte et l‚Äôingestion multi-source** des donn√©es, constituant la base de toute la cha√Æne data (ETL/ELT).

L‚Äôobjectif principal est de construire une **pipeline de collecte robuste, conteneuris√©e et modulaire**, capable d‚Äôing√©rer diff√©rentes sources de donn√©es :
- Donn√©es synth√©tiques (g√©n√©r√©es avec `Faker`)
- Donn√©es issues de fichiers plats (CSV)
- Donn√©es issues de **scraping web** avec `Selenium`
- Donn√©es temps r√©el provenant d‚Äô**APIs externes** (AviationStack, OpenWeather)
- Donn√©es temporaires et interm√©diaires stock√©es dans plusieurs bases (Redis, MySQL, Cassandra)
- Donn√©es en streaming envoy√©es vers un **cluster Kafka**

---

## Architecture Technique

###  Vue d‚Äôensemble

![Architecture de Collecte & Ingestion](assets/Collecte_injection.drawio.png)

Cette architecture illustre les diff√©rents canaux de collecte et leur int√©gration dans le pipeline Multi-Source.

###  Sources de donn√©es
| Source | Description | Technologie |
|--------|--------------|--------------|
| **FAKER** | G√©n√©ration de donn√©es a√©riennes et m√©t√©orologiques simul√©es | Python / Faker |
| **Fichiers CSV** | Donn√©es plats import√©es localement | Pandas / CSV |
| **Scraping** | Extraction de donn√©es web (vols, m√©t√©o) | Selenium |
| **APIs** | Donn√©es temps r√©el (m√©t√©o, trafic a√©rien) | OpenWeather, AviationStack |

###  Stockages interm√©diaires
| Base | R√¥le | Type |
|------|------|------|
| **Redis** | Cache et buffer des donn√©es brutes | NoSQL (Key-Value) |
| **MySQL** | Donn√©es structur√©es, persistantes | SQL (Relationnel) |
| **Cassandra** | Donn√©es volumineuses et distribu√©es | NoSQL (Colonne) |

### üîπ Pipeline Kafka
- **Kafka Producer Python** : publie les messages en temps r√©el dans le cluster
- **Kafka Cluster** : re√ßoit les donn√©es sur plusieurs *topics* (a√©rien, m√©t√©o)
- **Kafka Partitioning** : assure la scalabilit√© et la tol√©rance aux pannes

## Technologies utilis√©es

| Cat√©gorie | Outils / Librairies |
|------------|---------------------|
| Langage principal | Python 3 |
| Conteneurisation | Docker & Docker Compose |
| Streaming | Apache Kafka |
| Bases de donn√©es | MySQL, Redis, Cassandra |
| APIs externes | OpenWeather, AviationStack |
| G√©n√©ration de donn√©es | Faker |
| Web Scraping | Selenium |
| Gestion de d√©pendances | requirements.txt |
| Notebook d‚Äôanalyse | Jupyter (optionnel) |


## Lancer le projet avec Docker

### Cloner le d√©p√¥t
```bash
git clone https://github.com/<AssaneDiakhoumpa>/DataFlow360.git
cd DataFlow360
````

### Construire et lancer les conteneurs

```bash
docker-compose up --build
```

### V√©rifier les services

* Kafka Cluster : `localhost:9092`
* Redis : `localhost:6379`
* MySQL : `localhost:3306`
* Cassandra : `localhost:9042`


## Fonctionnement du pipeline

1. Les scripts `generate_*.py` g√©n√®rent des donn√©es simul√©es.
2. Les scrapers Selenium collectent des donn√©es r√©elles depuis le web.
3. Les API OpenWeather & AviationStack fournissent des donn√©es en temps r√©el.
4. Toutes ces donn√©es transitent par Redis / MySQL / Cassandra selon leur nature.
5. Les producteurs Kafka publient les messages dans le cluster Kafka.
6. Les topics Kafka peuvent ensuite √™tre consomm√©s pour les √©tapes suivantes :

   * Nettoyage
   * Transformation
   * Analyse / Machine Learning


## S√©curit√© & Environnement

* Les acc√®s API sont d√©finis dans le fichier `.env` (non partag√© publiquement).
* Chaque conteneur communique via un r√©seau Docker interne.
* Les donn√©es sont simul√©es ou publiques : **aucune donn√©e sensible** n‚Äôest collect√©e.


## Prochaines √©tapes

**√âtape 2 : Nettoyage & Transformation**

* Standardisation des sch√©mas de donn√©es
* V√©rification des types et valeurs manquantes
* Construction du Data Lake / Data Warehouse

**√âtape 3 : Orchestration & Monitoring**

* Int√©gration d‚ÄôAirflow pour planifier les pipelines
* Centralisation des logs et alertes (ELK / Prometheus)

## Extension Cloud Serverless : Ingestion temps r√©el (AWS simul√©e)

Cette section introduit une brique **Cloud Serverless**, simul√©e localement via **LocalStack**, afin de reproduire une architecture d‚Äôingestion temps r√©el typique d‚ÄôAWS.
Elle compl√®te le pipeline DataFlow360 par un flux **API ‚Üí Streaming ‚Üí Traitement ‚Üí Stockage** enti√®rement automatis√©.

### 1. Objectif

Mettre en place un pipeline temps r√©el bas√© sur :

* **Kinesis Stream** pour la collecte et le transport des donn√©es,
* **Lambda** pour le traitement serverless,
* **DynamoDB** pour le stockage NoSQL.

Flux g√©n√©ral :

```
OpenWeather API ‚Üí AWS Kinesis ‚Üí AWS Lambda ‚Üí AWS DynamoDB
```

---

### 2. Composants utilis√©s

| Composant                     | R√¥le                                                                                | Technologie     |
| ----------------------------- | ----------------------------------------------------------------------------------- | --------------- |
| **OpenWeather API**           | Source temps r√©el des donn√©es m√©t√©o (qualit√© de l‚Äôair, temp√©rature, humidit√©, etc.) | REST API        |
| **AWS Kinesis (LocalStack)**  | Collecte et transporte les messages entrants en streaming                           | Cloud Streaming |
| **AWS Lambda (LocalStack)**   | Fonction serverless qui consomme le flux Kinesis et √©crit dans DynamoDB             | Cloud Function  |
| **AWS DynamoDB (LocalStack)** | Base NoSQL pour le stockage persistant des donn√©es trait√©es                         | Cloud Database  |

### 3. Mise en place sur LocalStack

Avant de lancer les commandes suivantes, assurez-vous que :

* LocalStack est d√©marr√© via Docker,
* AWS CLI est configur√© avec le profil `awslocal`,
* Vous √™tes dans l‚Äôenvironnement virtuel Python avec les d√©pendances install√©es.

#### a. Cr√©er un flux Kinesis

```bash
awslocal kinesis create-stream \
  --stream-name weather_stream \
  --shard-count 1
```

V√©rifier :

```bash
awslocal kinesis list-streams
```

#### b. Cr√©er la table DynamoDB

```bash
awslocal dynamodb create-table \
  --table-name dataflow_mongo \
  --attribute-definitions AttributeName=ville,AttributeType=S \
  --key-schema AttributeName=ville,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST
```

V√©rifier :

```bash
awslocal dynamodb list-tables
```

---

#### c. Cr√©er la fonction Lambda

Compressez votre code Lambda :

```bash
zip lambda_consumer.zip lambda_consumer.py
```

Cr√©ez la fonction :

```bash
awslocal lambda create-function \
  --function-name kinesis_to_dynamo \
  --runtime python3.9 \
  --handler lambda_consumer.lambda_handler \
  --zip-file fileb://lambda_consumer.zip \
  --role arn:aws:iam::000000000000:role/lambda-role
```

V√©rifier :

```bash
awslocal lambda list-functions
```

---

#### d. Lier Kinesis √† Lambda

```bash
awslocal lambda create-event-source-mapping \
  --function-name kinesis_to_dynamo \
  --event-source arn:aws:kinesis:us-east-1:000000000000:stream/weather_stream \
  --batch-size 1 \
  --starting-position LATEST
```

V√©rifier la liaison :

```bash
awslocal lambda list-event-source-mappings \
  --function-name kinesis_to_dynamo
```

#### e. Tester le flux

Envoyer un message simul√© dans le flux :

```bash
aws --endpoint-url=http://localhost:4566 kinesis put-record \
  --stream-name weather_stream \
  --partition-key 1 \
  --data '{"ville": "Dakar", "pays": "S√©n√©gal", "aeroport": "DSS", "data": {"temp": 30, "pm2_5": 12}, "timestamp": 1731400000}'
```

Consulter les logs Lambda :

```bash
awslocal logs describe-log-groups
awslocal logs tail /aws/lambda/kinesis_to_dynamo --follow
```

V√©rifier l‚Äôinsertion dans DynamoDB :

```bash
awslocal dynamodb scan --table-name dataflow_mongo
```