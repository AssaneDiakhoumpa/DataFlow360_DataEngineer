#  Projet DataFlow360 ‚Äì Collecte & Ingestion Multi-Source

##  Contexte du projet

**DataFlow360** est une plateforme de traitement et d‚Äôanalyse de donn√©es a√©riennes et m√©t√©orologiques.  
Cette premi√®re phase du projet met l‚Äôaccent sur la **collecte et l‚Äôingestion multi-source** des donn√©es, constituant la base de toute la cha√Æne data (ETL/ELT).

L‚Äôobjectif principal est de construire une **pipeline de collecte robuste, conteneuris√©e et modulaire**, capable d‚Äôing√©rer diff√©rentes sources de donn√©es :
- Donn√©es synth√©tiques (g√©n√©r√©es avec `Faker`)
- Donn√©es issues de fichiers plats (CSV)
- Donn√©es issues de **scraping web** avec `Selenium`
- Donn√©es temps r√©el provenant d‚Äô**APIs externes** (AviationStack, OpenWeather)
- Donn√©es temporaires et interm√©diaires stock√©es dans plusieurs bases (Redis, MySQL, Cassandra)
- Donn√©es en streaming envoy√©es vers un **cluster Kafka**

---

## Architecture Technique

###  Vue d‚Äôensemble

![Architecture de Collecte & Ingestion](assets/Collecte_injection.drawio.png)

Cette architecture illustre les diff√©rents canaux de collecte et leur int√©gration dans le pipeline Multi-Source.

###  Sources de donn√©es
| Source | Description | Technologie |
|--------|--------------|--------------|
| **FAKER** | G√©n√©ration de donn√©es a√©riennes et m√©t√©orologiques simul√©es | Python / Faker |
| **Fichiers CSV** | Donn√©es plats import√©es localement | Pandas / CSV |
| **Scraping** | Extraction de donn√©es web (vols, m√©t√©o) | Selenium |
| **APIs** | Donn√©es temps r√©el (m√©t√©o, trafic a√©rien) | OpenWeather, AviationStack |

###  Stockages interm√©diaires
| Base | R√¥le | Type |
|------|------|------|
| **Redis** | Cache et buffer des donn√©es brutes | NoSQL (Key-Value) |
| **MySQL** | Donn√©es structur√©es, persistantes | SQL (Relationnel) |
| **Cassandra** | Donn√©es volumineuses et distribu√©es | NoSQL (Colonne) |

### üîπ Pipeline Kafka
- **Kafka Producer Python** : publie les messages en temps r√©el dans le cluster
- **Kafka Cluster** : re√ßoit les donn√©es sur plusieurs *topics* (a√©rien, m√©t√©o)
- **Kafka Partitioning** : assure la scalabilit√© et la tol√©rance aux pannes

## Technologies utilis√©es

| Cat√©gorie | Outils / Librairies |
|------------|---------------------|
| Langage principal | Python 3 |
| Conteneurisation | Docker & Docker Compose |
| Streaming | Apache Kafka |
| Bases de donn√©es | MySQL, Redis, Cassandra |
| APIs externes | OpenWeather, AviationStack |
| G√©n√©ration de donn√©es | Faker |
| Web Scraping | Selenium |
| Gestion de d√©pendances | requirements.txt |
| Notebook d‚Äôanalyse | Jupyter (optionnel) |


## Lancer le projet avec Docker

### Cloner le d√©p√¥t
```bash
git clone https://github.com/<AssaneDiakhoumpa>/DataFlow360.git
cd DataFlow360
````

### Construire et lancer les conteneurs

```bash
docker-compose up --build
```

### V√©rifier les services

* Kafka Cluster : `localhost:9092`
* Redis : `localhost:6379`
* MySQL : `localhost:3306`
* Cassandra : `localhost:9042`


## Fonctionnement du pipeline

1. Les scripts `generate_*.py` g√©n√®rent des donn√©es simul√©es.
2. Les scrapers Selenium collectent des donn√©es r√©elles depuis le web.
3. Les API OpenWeather & AviationStack fournissent des donn√©es en temps r√©el.
4. Toutes ces donn√©es transitent par Redis / MySQL / Cassandra selon leur nature.
5. Les producteurs Kafka publient les messages dans le cluster Kafka.
6. Les topics Kafka peuvent ensuite √™tre consomm√©s pour les √©tapes suivantes :

   * Nettoyage
   * Transformation
   * Analyse / Machine Learning


## S√©curit√© & Environnement

* Les acc√®s API sont d√©finis dans le fichier `.env` (non partag√© publiquement).
* Chaque conteneur communique via un r√©seau Docker interne.
* Les donn√©es sont simul√©es ou publiques : **aucune donn√©e sensible** n‚Äôest collect√©e.


## Prochaines √©tapes

**√âtape 2 : Nettoyage & Transformation**

* Standardisation des sch√©mas de donn√©es
* V√©rification des types et valeurs manquantes
* Construction du Data Lake / Data Warehouse

**√âtape 3 : Orchestration & Monitoring**

* Int√©gration d‚ÄôAirflow pour planifier les pipelines
* Centralisation des logs et alertes (ELK / Prometheus)
